{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Basics on Compression and the blosc2.NDArray Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import blosc2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic compress2/decompress2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a simple tensor\n",
    "a = np.arange(1000_000).reshape(1000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cbytes: 12732, cratio: 628.34x\n"
     ]
    }
   ],
   "source": [
    "compressed_a = blosc2.compress2(a)\n",
    "cbytes = len(compressed_a)\n",
    "nbytes = a.size * a.itemsize\n",
    "print(f\"cbytes: {cbytes}, cratio: {nbytes / cbytes:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(a2)=<class 'bytes'>, len(a2)=8000000\n"
     ]
    }
   ],
   "source": [
    "a2 = blosc2.decompress2(compressed_a)\n",
    "print(f\"{type(a2)=}, {len(a2)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     0,      1,      2, ...,    997,    998,    999],\n",
       "       [  1000,   1001,   1002, ...,   1997,   1998,   1999],\n",
       "       [  2000,   2001,   2002, ...,   2997,   2998,   2999],\n",
       "       ...,\n",
       "       [997000, 997001, 997002, ..., 997997, 997998, 997999],\n",
       "       [998000, 998001, 998002, ..., 998997, 998998, 998999],\n",
       "       [999000, 999001, 999002, ..., 999997, 999998, 999999]],\n",
       "      shape=(1000, 1000))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restore (requires effort and metainfo supplement)\n",
    "a3 = np.frombuffer(a2, dtype=np.int64).reshape(1000, 1000)\n",
    "a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More fancy compression for tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cbytes: 13000, cratio: 615.38x\n"
     ]
    }
   ],
   "source": [
    "compressed_t = blosc2.pack_tensor(a)\n",
    "cbytes = len(compressed_t)\n",
    "print(f\"cbytes: {cbytes}, cratio: {nbytes / cbytes:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(a4)=<class 'numpy.ndarray'>, len(a4)=1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[     0,      1,      2, ...,    997,    998,    999],\n",
       "       [  1000,   1001,   1002, ...,   1997,   1998,   1999],\n",
       "       [  2000,   2001,   2002, ...,   2997,   2998,   2999],\n",
       "       ...,\n",
       "       [997000, 997001, 997002, ..., 997997, 997998, 997999],\n",
       "       [998000, 998001, 998002, ..., 998997, 998998, 998999],\n",
       "       [999000, 999001, 999002, ..., 999997, 999998, 999999]],\n",
       "      shape=(1000, 1000))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restore (fancier)\n",
    "a4 = blosc2.unpack_tensor(compressed_t)\n",
    "print(f\"{type(a4)=}, {len(a4)=}\")\n",
    "a4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression params\n",
    "Let's see how to copy the NDArray data whilst altering the compression parameters. This may be useful in many contexts, for example testing how changing the codec of an existing array affects the compression ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cbytes: 21569, cratio: 370.90x\n"
     ]
    }
   ],
   "source": [
    "cparams = blosc2.CParams(\n",
    "    codec=blosc2.Codec.LZ4,\n",
    "    clevel=9,\n",
    "    filters=[blosc2.Filter.SHUFFLE, blosc2.Filter.BYTEDELTA],\n",
    ")\n",
    "\n",
    "compressed_t = blosc2.pack_tensor(a, cparams=cparams)\n",
    "cbytes = len(compressed_t)\n",
    "print(f\"cbytes: {cbytes}, cratio: {nbytes / cbytes:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the compression ratio is quite lower than before, since we have changed to a different codec that is optimised for compression speed, not compression ratio. In general there is a tradeoff between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native Blosc2 codecs\n",
    "Blosc2 supports many standard codecs, since there is no one-size-fits-all compression solution - one codec may be perfect for one context, but quite suboptimal in another.\n",
    "* ZLIB codec: uses the DEFLATE algorithm, is standard, and works well for images.\n",
    "* ZSTD codec: better compression ratio to ZLIB and faster compression/decompression (the default in Blosc2).\n",
    "* LZ4 codec: faster comp/decomp than ZSTD but reduced compression ratio.\n",
    "* BloscLZ: new implementation of the simple FASTLZ algorithm; similar tradeoff to LZ4, but the latter is generally better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Use different codecs (BLOSCLZ, LZ4, LZ4HC, ZLIB and ZSTD), clevels (0-9) and filters (SHUFFLE, BITSHUFFLE, BYTEDELTA) above and see how cratios vary.\n",
    "\n",
    "Hint: the BYTEDELTA filter is meant to be applied after a SHUFFLE filter; otherwise, the results will be sub-optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cbytes: 982, cratio: 8146.64x\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "cparams = blosc2.CParams(\n",
    "    codec=blosc2.Codec.ZSTD,\n",
    "    clevel=9,\n",
    "    filters=[blosc2.Filter.SHUFFLE, blosc2.Filter.BYTEDELTA],\n",
    ")\n",
    "\n",
    "compressed_t = blosc2.pack_tensor(a, cparams=cparams)\n",
    "cbytes = len(compressed_t)\n",
    "print(f\"cbytes: {cbytes}, cratio: {nbytes / cbytes:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, via package extensions to Blosc2, one may access the JPEG2000 family of compression algorithms, which aim for a compromise between compression ratio and image quality; Blosc2 implements plugins for [GROK](https://github.com/Blosc/blosc2_grok) and [OPENHTJ2K](https://github.com/Blosc/blosc2_openhtj2k) for a convenient way to access JPEG2000 compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDArray: A NDim, Compressed Data Container\n",
    "\n",
    "NDArray objects let users perform different operations with  arrays like setting, copying or slicing them. In this section, we are going to see how to create and manipulate these NDArray arrays, which possess metadata and data. The data is *chunked* and *compressed*; the metadata gives information about the data itself, as well as the chunking and compression. Chunking and compression are features which make NDArray arrays very efficient for working with large data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an array\n",
    "Let's start by creating a 2D array with 100M elements filled with ``arange``. We can then print out the metadata, which contains information about: the array data (such as ``shape`` and ``dtype``); and how the data is compressed and stored, such as chunk- and block-shapes (``chunks`` and ``blocks``) and compression params (``CParams``). See [here](https://www.blosc.org/python-blosc2/getting_started/overview.html) for an explanation of chunking and blocking.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type    : NDArray\n",
      "shape   : (10000, 10000)\n",
      "chunks  : (1250, 10000)\n",
      "blocks  : (10, 10000)\n",
      "dtype   : int32\n",
      "nbytes  : 400000000\n",
      "cbytes  : 1093039\n",
      "cratio  : 365.95\n",
      "cparams : CParams(codec=<Codec.ZSTD: 5>, codec_meta=0, clevel=5, use_dict=False, typesize=4,\n",
      "        : nthreads=8, blocksize=400000, splitmode=<SplitMode.AUTO_SPLIT: 3>,\n",
      "        : filters=[<Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>,\n",
      "        : <Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>, <Filter.SHUFFLE: 1>], filters_meta=[0, 0,\n",
      "        : 0, 0, 0, 0], tuner=<Tuner.STUNE: 0>)\n",
      "dparams : DParams(nthreads=8)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shape = (10_000, 10_000)\n",
    "array = blosc2.arange(np.prod(shape), shape=shape, dtype=np.int32)\n",
    "print(array.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``cratio`` parameter tells us how effective the compression is, since it gives the ratio between the number of bytes required to store the array in uncompressed and compressed form. Here we require almost 300x less space for the compressed array! Note that all the compression and decompression parameters are set to the default, and ``chunks`` and ``blocks`` have been selected automatically - playing around with them will affect the ``cratio`` (as well as compression and decompression speed).\n",
    "\n",
    "We can also create an NDArray by compressing a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type    : NDArray\n",
      "shape   : (10000, 10000)\n",
      "chunks  : (625, 10000)\n",
      "blocks  : (5, 10000)\n",
      "dtype   : float64\n",
      "nbytes  : 800000000\n",
      "cbytes  : 14833410\n",
      "cratio  : 53.93\n",
      "cparams : CParams(codec=<Codec.ZSTD: 5>, codec_meta=0, clevel=5, use_dict=False, typesize=8,\n",
      "        : nthreads=8, blocksize=400000, splitmode=<SplitMode.AUTO_SPLIT: 3>,\n",
      "        : filters=[<Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>,\n",
      "        : <Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>, <Filter.SHUFFLE: 1>], filters_meta=[0, 0,\n",
      "        : 0, 0, 0, 0], tuner=<Tuner.STUNE: 0>)\n",
      "dparams : DParams(nthreads=8)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nparray = np.linspace(0, 100, np.prod(shape), dtype=np.float64).reshape(shape)\n",
    "b2array = blosc2.asarray(nparray)\n",
    "print(b2array.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or an iterator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type    : NDArray\n",
      "shape   : (1000000,)\n",
      "chunks  : (1000000,)\n",
      "blocks  : (62500,)\n",
      "dtype   : [('f0', '<i4'), ('f1', '<f4'), ('f2', '<f8')]\n",
      "nbytes  : 16000000\n",
      "cbytes  : 7060260\n",
      "cratio  : 2.27\n",
      "cparams : CParams(codec=<Codec.ZSTD: 5>, codec_meta=0, clevel=5, use_dict=False, typesize=16,\n",
      "        : nthreads=8, blocksize=1000000, splitmode=<SplitMode.AUTO_SPLIT: 3>,\n",
      "        : filters=[<Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>,\n",
      "        : <Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>, <Filter.SHUFFLE: 1>], filters_meta=[0, 0,\n",
      "        : 0, 0, 0, 0], tuner=<Tuner.STUNE: 0>)\n",
      "dparams : DParams(nthreads=8)\n",
      "\n",
      "first 3 rows of sa: [( 1, -2., 0.71085376) ( 0, -1., 0.1804048 ) (-1,  0., 0.48975023)]\n"
     ]
    }
   ],
   "source": [
    "N = 1000_000\n",
    "rng = np.random.default_rng()\n",
    "it = ((-x + 1, x - 2, rng.normal()) for x in range(N))\n",
    "sa = blosc2.fromiter(it, dtype=\"i4,f4,f8\", shape=(N,))\n",
    "print(sa.info)\n",
    "print(f\"first 3 rows of sa: {sa[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create a 2D NDArray with shape (1000, 10_000) and filled with sequential integers using the `range` iterator. Then use `blosc2.arange` to create the same array, and check that the two arrays are equal.  Use `%time` magick tool to time the two operations.  What do you notice about the time taken?  Why do you think this is happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.16 s, sys: 88.3 ms, total: 1.25 s\n",
      "Wall time: 1.1 s\n",
      "CPU times: user 129 ms, sys: 18 ms, total: 147 ms\n",
      "Wall time: 89 ms\n",
      "CPU times: user 3.61 ms, sys: 2.5 ms, total: 6.11 ms\n",
      "Wall time: 6.12 ms\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "%time array_from_iter = blosc2.fromiter(range(1000 * 10_000), dtype='i4', shape=(1000, 10_000))\n",
    "%time array_from_arange = blosc2.arange(10_000_000, shape=(1000, 10_000))\n",
    "# For reference\n",
    "%time array_from_arange = np.arange(10_000_000).reshape((1000, 10_000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Reading and modifying data\n",
    "NDArray arrays cannot be read directly, since they are compressed, and so must be decompressed first (to NumPy arrays, which are stored in memory). This can be done for the full array using the ``[:]`` operator, which returns a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = array[:]  # This will decompress the full array\n",
    "type(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "However it is often not necessary (or desirable) to load the whole array into memory. We can easily read just small parts of NDArray arrays to a NumPy array, quickly, via standard indexing routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got one element (of shape (10000,)) and slice of shape (4, 10000).\n",
      "[   0    1    2 ... 9997 9998 9999]\n",
      "[[60000 60001 60002 ... 69997 69998 69999]\n",
      " [70000 70001 70002 ... 79997 79998 79999]\n",
      " [80000 80001 80002 ... 89997 89998 89999]\n",
      " [90000 90001 90002 ... 99997 99998 99999]]\n"
     ]
    }
   ],
   "source": [
    "res1 = array[0]  # get first element\n",
    "res2 = array[6:10]  # get slice\n",
    "print(f\"Got one element (of shape {res1.shape}) and slice of shape {res2.shape}.\")\n",
    "print(res1)\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can modify the data in the array using standard NumPy indexing too, using either NumPy or NDArray arrays as the data source.  For example, we can set the first row to zeros (using an NDArray array) and the first column to ones (using a NumPy array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<blosc2.ndarray.NDArray object at 0x1089c3510>\n"
     ]
    }
   ],
   "source": [
    "array[0, :] = blosc2.zeros(10000, dtype=array.dtype)\n",
    "array[:, 0] = np.ones(10000, dtype=array.dtype)\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ``array`` is still an NDArray array. Let's check that the entries were correctly modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1 0 0 ... 0 0 0]\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(array[0, 0])\n",
    "print(array[0, :])\n",
    "print(array[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enlarging the array\n",
    "Existing arrays can be enlarged. This is one operation that is greatly enhanced by the chunking procedure implemented in NDArray arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 μs, sys: 1e+03 ns, total: 19 μs\n",
      "Wall time: 21 μs\n",
      "(10001, 10000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], shape=(10000,), dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time array.resize((10_001, 10_000))\n",
    "print(array.shape)\n",
    "array[10_000, :] = 1\n",
    "array[10_000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.9 ms, sys: 82.6 ms, total: 139 ms\n",
      "Wall time: 139 ms\n"
     ]
    }
   ],
   "source": [
    "%time nparray2 = np.resize(nparray, (10_001, 10_001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enlarging a NumPy array requires a full copy of the data, since underlying data are stored contiguously in memory, which is very costly: new memory to hold the extended array is allocated, the old data is copied to part of the new memory, and then the new data is written to the remaining new memory.\n",
    "Enlarging is a much faster operation for NDArray arrays because data is chunked, and the chunks may be stored non-contiguously in memory, so one may simply write the necessary new chunks to some arbitrary address in memory and leave the old chunks untouched. The references to the new chunk addresses are then added in the NDArray container, which is a very quick operation.\n",
    "\n",
    "You can also shrink the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 10000)\n",
      "[       1 89990001 89990002 ... 89999997 89999998 89999999]\n"
     ]
    }
   ],
   "source": [
    "array.resize((9_000, 10_000))\n",
    "print(array.shape)\n",
    "print(array[8_999])  # This works\n",
    "# array[9_000]  # This will raise an exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistent data\n",
    "We can use the `save()` method to store the array on disk.  This is very useful when you are working with a large array but do not need to access it often.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--@ 1 faltet  staff   1,2M 10 des.  08:33 array_tutorial.b2nd\n"
     ]
    }
   ],
   "source": [
    "array.save(\"array_tutorial.b2nd\", mode=\"w\")  # , contiguous=True)\n",
    "!ls -lh array_tutorial.b2nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For arrays, it is usual to use the `.b2nd` extension. Now let's open the saved array and check that the data saved correctly (decompressing first to be able to compare):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array2 = blosc2.open(\"array_tutorial.b2nd\")\n",
    "np.array_equal(array2, array) # Make sure saved array matches original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the ``urlpath`` parameter, we can also write directly to disk using the other constructors we saw previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 first rows of arr: [[    0     1     2 ...  9997  9998  9999]\n",
      " [10000 10001 10002 ... 19997 19998 19999]\n",
      " [20000 20001 20002 ... 29997 29998 29999]]\n",
      "3 first rows of sa: [( 1, -2., -0.61881834) ( 0, -1., -0.21309748) (-1,  0., -0.39536403)]\n",
      "3 first rows of b2array: [[0.00000000e+00 1.00000001e-06 2.00000002e-06 ... 9.99700010e-03\n",
      "  9.99800010e-03 9.99900010e-03]\n",
      " [1.00000001e-02 1.00010001e-02 1.00020001e-02 ... 1.99970002e-02\n",
      "  1.99980002e-02 1.99990002e-02]\n",
      " [2.00000002e-02 2.00010002e-02 2.00020002e-02 ... 2.99970003e-02\n",
      "  2.99980003e-02 2.99990003e-02]]\n"
     ]
    }
   ],
   "source": [
    "arr = blosc2.arange(np.prod(shape), shape=shape, dtype=np.int32, urlpath=\"arange.b2nd\", mode=\"w\")\n",
    "print(\"3 first rows of arr:\", arr[:3])\n",
    "it = ((-x + 1, x - 2, rng.normal()) for x in range(N))\n",
    "sa = blosc2.fromiter(it, dtype=\"i4,f4,f8\", shape=(N,), urlpath=\"sa-1M.b2nd\", mode=\"w\")\n",
    "print(\"3 first rows of sa:\", sa[:3])\n",
    "b2array = blosc2.asarray(nparray, urlpath=\"linspace.b2nd\", mode=\"w\")\n",
    "print(\"3 first rows of b2array:\", b2array[:3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
